---
title: "project_part_1"
author: "Annie Zhu"
date: "2023-01-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
```

## Data Cleaning

Here I will be adding an extra row for easier understanding of what every value represents. More specifically, I am taking the description of what each attribute means from the repository description online and converting it into variable names for each column.

```{r blogset}
data <- read_csv("./BlogFeedback/blogData_train.csv", col_names = FALSE, show_col_types = FALSE)
basic_attributes <- c("total_before", "before_24hr", "from_48hr_to_24hr", "first_24hr", "change_between_two_days")
prefix <- c("avg", "sd", "min", "max", "med")
suffix <- c("comments", "links")
for (i in 1:5){
  for (j in 1:2) {
    for (k in 1:5) {
      index = 10 * (i-1) + 5 * (j-1) + k
      colnames(data)[index] <- paste(prefix[k], basic_attributes[i], suffix[j], sep = "_")
    }
  }
}

for (i in 1:5){
  for (j in 1:2) {
    index = 50 + i + 5 * (j-1)
    colnames(data)[index] <- paste(basic_attributes[i], suffix[j], sep = "_")
  }
}

colnames(data)[61] <- "time_length"
colnames(data)[62] <- "post_length"
for (i in 1: 200){
  index = toString(i)
  colnames(data)[i + 62] <- paste("freq_word_feature_", index, sep = "")
}

days <- c("mon", "tues", "wed", "thurs", "fri", "sat", "sun")
for (i in 1:7) {
  colnames(data)[i + 262] <- paste("basetime", days[i], sep = "_")
}
for (i in 1:7) {
  colnames(data)[i + 269] <- paste("publication", days[i], sep = "_")
}

colnames(data)[277] <- "parent_pages_count"
colnames(data)[278] <- "min_parent_pages_comments"
colnames(data)[279] <- "max_parent_pages_comments"
colnames(data)[280] <- "avg_parent_pages_comments"
colnames(data)[281] <- "after_24hr_comments"
print(data)


```

Next I also want to count the number of rows cumulatively across the 60 different test datasets to help determine if we want to change the train-test split.

```{r testCount}
test_count <- 0
numbers <- seq(1, 31)
numbers <- sprintf("%02d", numbers)

for (i in 1:29) {
  path_name <- paste("./BlogFeedback/blogData_test-2012.02.", numbers[i], ".00_00.csv", sep = "")
  test_data <- read_csv(path_name, col_names = FALSE, show_col_types = FALSE)
  test_count <- test_count + nrow(test_data)
}


for (i in 1:25) {
  path_name <- paste("./BlogFeedback/blogData_test-2012.03.", numbers[i], ".00_00.csv", sep = "")
  test_data <- read_csv(path_name, col_names = FALSE, show_col_types = FALSE)
  test_count <- test_count + nrow(test_data)
}

for (i in 26:31) {
  path_name <- paste("./BlogFeedback/blogData_test-2012.03.", numbers[i], ".01_00.csv", sep = "")
  test_data <- read_csv(path_name, col_names = FALSE, show_col_types = FALSE)
  test_count <- test_count + nrow(test_data)
}

print(test_count)
print(nrow(data))

```

This means just from the way the data set was split from the repository, we would have 7624 rows for testing and 52397 rows for training. This creates about a 87\% \/ 13\% split between the training and testing data respectively, but typically a 80\% \/ 20\% split is used. Since we have 60021 instances in total, it would make sense for us to try to balance out the split so that there is more testing data, so later we can have more confidence in our generalized error calculation. The easiest way for us to achieve that 80\% \/ 20\% split is for us to randomly select 21901 rows from the initial training dataset to NOT include in our analysis. That way we would have 7624 rows for testing, 30496 rows for training, and 38120 instances in total. 

With this new split and removal of some training data, we are still well above the minimum requirement for number of rows, and we do not have to worry at all about whether the train \/ test split is getting messed up by fact that some of this data relies on time covariates (i.e. 24 hrs before, 48hrs before) because we are not transitioning training data into testing data.

## Data Transformation
Let's add in some new columns. 1) the binary output 2) day of the week categories.



Also, let's delete columns that seem to add no value in at all because they are almost all zeros.

## Data Exploration

Explroing patterns for binary prediction variable:

```{r exploration, echo = FALSE}

ggplot(data = data, mapping = aes(x = total_before_comments, y = after_24hr_comments - before_24hr_comments)) + 
  geom_point(mapping = aes(color = time_length)) +
  labs(title = "Change in comments between Day 1 and Day 2 vs Day 2 and Day 3")

```
```{r exploration1, echo = FALSE}

ggplot(data = data, mapping = aes(x = change_between_two_days_comments, y = after_24hr_comments - before_24hr_comments)) + 
  geom_point(mapping = aes(color = time_length)) +
  labs(title = "Change in Number of Comments Across Two Days Before and After Basetime",
       x = "D2 - D1",
       y = "D3 - D2",
       color = "Age of Blog Post")

```

Checking for correlation between variables:

```{r exploration3, echo = FALSE}

ggplot(data = data, mapping = aes(x = time_length, y = after_24hr_comments)) + 
  geom_point(mapping = aes(color = before_24hr_comments)) +
  labs(title = "Time vs Comments Outcome")

ggplot(data = data, mapping = aes(x = time_length, y = after_24hr_comments - before_24hr_comments)) + 
  geom_point(mapping = aes(color = before_24hr_comments)) +
  labs(title = "Time vs Comments Outcome")

```


```{r exploration4, echo = FALSE}

ggplot(data = data, mapping = aes(x = before_24hr_comments, y = after_24hr_comments)) + 
  geom_point() +
  geom_smooth(se = FALSE) + 
  labs(title = "24 hours before vs after")

ggplot(data = data, mapping = aes(x = avg_before_24hr_comments, y = after_24hr_comments)) + 
  geom_point() +
  geom_smooth(se = FALSE) + 
  labs(title = "24 hours before vs after")

```


```{r exploration3, echo = FALSE}

ggplot(data = data, mapping = aes(x = before_24hr_comments, y = avg_before_24hr_comments)) + 
  geom_point() +
  labs(title = "Time vs Comments Outcome")

```
